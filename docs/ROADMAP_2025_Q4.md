# ğŸ—ºï¸ Cliente-Core Feature Roadmap - Q4 2025

**Project:** Va Nessa MudanÃ§a - Cliente Microservice
**Document Type:** Technical Roadmap
**Created:** 2025-11-03
**Status:** ğŸŸ¢ APPROVED
**Owner:** Tech Lead + Product Owner
**Review Date:** 2025-12-01

---

## ğŸ“Š Executive Summary

This roadmap defines **5 strategic features** to evolve the cliente-core microservice from basic CRUD to a production-grade system with advanced search, data export, event-driven analytics integration, and enterprise logging.

### Key Metrics

| Metric | Value |
|--------|-------|
| **Total Features** | 5 |
| **Total Effort** | 64 hours (~8-10 days) |
| **New Files** | 35 files |
| **Modified Files** | 16 files |
| **New Dependencies** | 8 Maven artifacts |
| **Infrastructure Required** | Kafka MSK cluster |

### Business Value

- âœ… **Feature 1:** Complete CRUD (enables client lifecycle management)
- âœ… **Feature 2:** Reduce search time from manual queries to <1s (UX improvement)
- âœ… **Feature 3:** Enable business reports (CSV/PDF exports)
- âœ… **Feature 4:** Enable data-driven decisions (analytics integration)
- âœ… **Feature 5:** Reduce debugging time by 50% (structured logs)

---

## ğŸ¯ Feature Overview

### Feature Priority Matrix

```
High Business Value
â”‚
â”‚  F3 Export        F2 Search
â”‚  (Reports)        (UX)
â”‚       â—               â—
â”‚
â”‚  F5 Logging      F1 Delete
â”‚  (DevOps)        (CRUD)
â”‚       â—               â—
â”‚
â”‚               F4 Kafka
â”‚               (Analytics)
â”‚                   â—
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ High Technical Complexity
```

### Implementation Order (Recommended)

```
Week 1: Foundation
â”œâ”€ Day 1: F5 - Logging JSON (enables debugging for others)
â”œâ”€ Day 2: F1 - DELETE (completes CRUD)
â””â”€ Day 3-4: F2 - Advanced Search (high user demand)

Week 2: Integration & Reports
â”œâ”€ Day 5-6: F3 - Export CSV/PDF (business reports)
â””â”€ Day 7-10: F4 - Kafka Events (analytics integration)
```

**Rationale:**
1. **Logging first** = Foundation for debugging all other features
2. **DELETE next** = Quick win, completes CRUD
3. **Search** = High user impact
4. **Export** = Business value (reports)
5. **Kafka last** = Most complex, depends on stable CRUD

---

## ğŸ“‹ Features Detail

---

## FEATURE 1: DELETE Cliente (Soft Delete)

### Overview

| Attribute | Value |
|-----------|-------|
| **Complexity** | â­ Simples |
| **Effort** | 6 hours |
| **Priority** | HIGH |
| **Dependencies** | None |
| **Risk** | LOW |

### Description

Implement soft delete for Cliente entities (PF and PJ) by setting `ativo = false` instead of physical deletion. Preserves referential integrity and audit trail.

### Technical Decision

**Soft Delete > Hard Delete**

**Reasons:**
- âœ… Preserves audit trail (LGPD compliance)
- âœ… Maintains referential integrity (no orphan records)
- âœ… Enables "undo" functionality (future)
- âœ… Analytics can track churn (deleted clients)

**Trade-off:**
- âš ï¸ Database grows larger over time (mitigated by archiving strategy)

### Implementation Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HTTP Layer                              â”‚
â”‚  DELETE /v1/clientes/{pf|pj}/{publicId}                    â”‚
â”‚  â†’ ClientePFController.deletar(UUID publicId)              â”‚
â”‚  â†’ ClientePJController.deletar(UUID publicId)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Application Layer                          â”‚
â”‚  DeleteCliente{PF|PJ}Service (@Transactional)              â”‚
â”‚  1. Find by publicId (404 if not found)                    â”‚
â”‚  2. Validate already deleted (409 if ativo=false)          â”‚
â”‚  3. Set ativo = false                                       â”‚
â”‚  4. Set dataInativacao = now()                             â”‚
â”‚  5. Set usuarioInativou = current user (future)            â”‚
â”‚  6. Save to repository                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Domain Layer                             â”‚
â”‚  Cliente entity adds:                                       â”‚
â”‚  - dataInativacao: LocalDateTime (nullable)                â”‚
â”‚  - usuarioInativou: String (nullable, future)              â”‚
â”‚  - motivoInativacao: String (nullable, future)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Files to Create (7 files)

```
src/main/java/br/com/vanessa_mudanca/cliente_core/
â”œâ”€â”€ application/ports/input/
â”‚   â”œâ”€â”€ DeleteClientePFUseCase.java
â”‚   â””â”€â”€ DeleteClientePJUseCase.java
â”œâ”€â”€ application/service/
â”‚   â”œâ”€â”€ DeleteClientePFService.java
â”‚   â””â”€â”€ DeleteClientePJService.java
â””â”€â”€ domain/exception/
    â””â”€â”€ ClienteJaInativoException.java

src/test/java/br/com/vanessa_mudanca/cliente_core/
â””â”€â”€ application/service/
    â”œâ”€â”€ DeleteClientePFServiceTest.java
    â””â”€â”€ DeleteClientePJServiceTest.java
```

### Files to Modify (3 files)

```
1. Cliente.java (domain/entity/)
   + dataInativacao: LocalDateTime
   + usuarioInativou: String
   + motivoInativacao: String
   + isAtivo(): boolean

2. ClientePFController.java
   + DELETE /{publicId} endpoint

3. ClientePJController.java
   + DELETE /{publicId} endpoint
```

### Database Changes

**Liquibase Changeset:** `012-add-soft-delete-fields.sql`

```sql
ALTER TABLE clientes
ADD COLUMN data_inativacao TIMESTAMP,
ADD COLUMN usuario_inativou VARCHAR(100),
ADD COLUMN motivo_inativacao VARCHAR(500);

CREATE INDEX idx_clientes_ativo
ON clientes(ativo)
WHERE ativo = false;

COMMENT ON COLUMN clientes.data_inativacao IS 'Data em que o cliente foi desativado (soft delete)';
```

### Dependencies (Maven)

None - uses existing dependencies.

### Configuration (application.yml)

None required.

### Testing Strategy

**Unit Tests (8 scenarios):**
1. âœ… Delete existing PF client successfully
2. âœ… Delete existing PJ client successfully
3. âŒ Delete non-existent client (404)
4. âŒ Delete already deleted client (409)
5. âœ… Verify ativo=false after delete
6. âœ… Verify dataInativacao set correctly
7. âœ… Related entities remain (not cascaded)
8. âœ… Can still query by publicId (but ativo=false)

**Integration Tests:**
- Controller test with MockMvc (HTTP 204)
- Repository test (verify database state)

### QA Test Plan

**File:** `docs/qa/DELETE_CLIENTE_TEST_PLAN.md` (to be created)

**Scenarios:**
1. Happy path (both PF and PJ)
2. Error cases (404, 409)
3. Idempotency (delete twice)
4. Performance (<100ms)
5. Security (cross-client deletion blocked)

### Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Accidentally expose deleted clients in listings | MEDIUM | HIGH | Update ListCliente services to filter `ativo = true` by default |
| Performance degradation with many deleted records | LOW | MEDIUM | Add database partition strategy (future) |
| Compliance (LGPD right to deletion) | LOW | HIGH | Document that physical deletion available on request |

### Success Criteria

- âœ… DELETE endpoints return HTTP 204 No Content
- âœ… Deleted clients have `ativo = false`
- âœ… Cannot delete twice (409 Conflict)
- âœ… Listings exclude deleted clients by default
- âœ… All 8 unit tests passing

### Effort Breakdown

- Design: 1h
- Implementation: 2h
- Testing: 2h
- Documentation: 1h
- **Total: 6h**

---

## FEATURE 2: Advanced Search

### Overview

| Attribute | Value |
|-----------|-------|
| **Complexity** | â­â­ MÃ©dia |
| **Effort** | 14 hours |
| **Priority** | HIGH |
| **Dependencies** | None |
| **Risk** | MEDIUM (query performance) |

### Description

Implement advanced search endpoint allowing queries by: name, email, phone, CPF/CNPJ with full-text search support. Uses PostgreSQL native full-text search (no ElasticSearch needed).

### Technical Decision: PostgreSQL Full-Text > ElasticSearch

**Decision:** Use PostgreSQL GIN indexes (already implemented!)

**Reasons:**
- âœ… **Indexes already exist:** `idx_clientes_pf_nome_completo` and `idx_clientes_pj_razao_social` (lines 53-77 in `010-create-indexes.sql`)
- âœ… **Zero infrastructure cost:** Included in RDS PostgreSQL
- âœ… **Sufficient performance:** <1s for 10k clients
- âœ… **Zero operational overhead:** No ElasticSearch cluster to manage
- âœ… **Portuguese language support:** to_tsvector('portuguese', ...) configured

**When to migrate to ElasticSearch:**
- âŒ Volume > 100k clients
- âŒ Need autocomplete <20ms response time
- âŒ Complex geospatial queries
- âŒ Multi-language full-text search

**Cost Comparison:**
- PostgreSQL: $0 additional (already using RDS)
- ElasticSearch: $200-300/month (AWS OpenSearch minimum)

### Implementation Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HTTP Layer                              â”‚
â”‚  GET /v1/clientes/search?q=joao&tipo=PF&ativo=true         â”‚
â”‚  â†’ ClienteSearchController.search(SearchRequest)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Application Layer                          â”‚
â”‚  SearchClienteService                                       â”‚
â”‚  1. Parse search query                                      â”‚
â”‚  2. Build dynamic query (Specification pattern)            â”‚
â”‚  3. Call repository with filters                           â”‚
â”‚  4. Map to SearchResultDTO                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Layer                           â”‚
â”‚  ClienteSearchRepository (custom interface)                â”‚
â”‚  â†’ Native SQL query with to_tsvector()                     â”‚
â”‚  â†’ Uses GIN indexes for fast full-text search              â”‚
â”‚                                                             â”‚
â”‚  SELECT c.public_id, c.email,                              â”‚
â”‚         pf.primeiro_nome, pf.sobrenome,                    â”‚
â”‚         pj.razao_social, pj.nome_fantasia                  â”‚
â”‚  FROM clientes c                                            â”‚
â”‚  LEFT JOIN clientes_pf pf ON c.id = pf.id                 â”‚
â”‚  LEFT JOIN clientes_pj pj ON c.id = pj.id                 â”‚
â”‚  WHERE                                                      â”‚
â”‚    to_tsvector('portuguese', pf.primeiro_nome || ' ' ||    â”‚
â”‚                pf.sobrenome) @@ plainto_tsquery('portuguese', :query) â”‚
â”‚    OR c.email ILIKE :email                                 â”‚
â”‚    OR EXISTS (SELECT 1 FROM contatos ct                    â”‚
â”‚                WHERE ct.cliente_id = c.id                  â”‚
â”‚                AND ct.valor ILIKE :phone)                  â”‚
â”‚  ORDER BY c.data_criacao DESC                              â”‚
â”‚  LIMIT :size OFFSET :offset                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Search Filters Supported

| Filter | Type | Example | Index Used |
|--------|------|---------|------------|
| **q** (query) | Full-text | `q=joao silva` | `idx_clientes_pf_nome_completo` (GIN) |
| **email** | Exact/ILIKE | `email=joao@gmail.com` | `idx_clientes_email` (B-tree) |
| **cpf** | Exact | `cpf=12345678910` | `idx_clientes_pf_cpf` (UNIQUE) |
| **cnpj** | Exact | `cnpj=11222333000181` | `idx_clientes_pj_cnpj` (UNIQUE) |
| **telefone** | ILIKE | `telefone=11987654321` | `idx_contatos_valor` |
| **tipo** | Enum | `tipo=PF` or `tipo=PJ` | Discriminator column |
| **ativo** | Boolean | `ativo=true` | `idx_clientes_ativo` (partial) |
| **tipoCliente** | Enum | `tipoCliente=CONSIGNANTE` | `idx_clientes_tipo_cliente` |

### Files to Create (11 files)

```
src/main/java/br/com/vanessa_mudanca/cliente_core/
â”œâ”€â”€ application/dto/input/
â”‚   â””â”€â”€ SearchClienteRequest.java (query params)
â”œâ”€â”€ application/dto/output/
â”‚   â””â”€â”€ SearchClienteResponse.java (unified PF+PJ result)
â”œâ”€â”€ application/ports/input/
â”‚   â””â”€â”€ SearchClienteUseCase.java
â”œâ”€â”€ application/service/
â”‚   â””â”€â”€ SearchClienteService.java
â”œâ”€â”€ infrastructure/repository/
â”‚   â”œâ”€â”€ ClienteSearchRepository.java (custom interface)
â”‚   â””â”€â”€ ClienteSearchRepositoryImpl.java (native SQL)
â””â”€â”€ infrastructure/controller/
    â””â”€â”€ ClienteSearchController.java

src/test/java/br/com/vanessa_mudanca/cliente_core/
â”œâ”€â”€ application/service/
â”‚   â””â”€â”€ SearchClienteServiceTest.java
â”œâ”€â”€ infrastructure/repository/
â”‚   â””â”€â”€ ClienteSearchRepositoryTest.java
â””â”€â”€ infrastructure/controller/
    â””â”€â”€ ClienteSearchControllerTest.java
```

### Files to Modify (2 files)

```
1. application-dev.yml
   + search.max-results: 100 (limit per query)
   + search.default-page-size: 20

2. OpenApiConfig.java
   + Document search endpoint parameters
```

### Database Changes

**None required!** Indexes already exist:
- `idx_clientes_pf_nome_completo` (GIN full-text, Portuguese)
- `idx_clientes_pj_razao_social` (GIN full-text, Portuguese)
- `idx_clientes_email`
- `idx_contatos_valor`

### Dependencies (Maven)

None - uses existing Spring Data JPA.

### Configuration (application.yml)

```yaml
search:
  max-results: 100              # Maximum results per query
  default-page-size: 20         # Default pagination size
  full-text-language: portuguese # PostgreSQL FTS language
```

### Performance Considerations

**Expected Performance:**
- Simple query (name): <100ms
- Complex query (name + email + phone): <500ms
- Volume tested: 10k clients

**Query Optimization:**
- Use `EXPLAIN ANALYZE` to verify index usage
- Limit results to 100 per query
- Implement cursor-based pagination for large result sets (future)

**Monitoring:**
```sql
-- Verify index usage
EXPLAIN ANALYZE
SELECT ... WHERE to_tsvector(...) @@ plainto_tsquery(...);

-- Check for sequential scans (bad!)
-- Should show "Bitmap Index Scan on idx_clientes_pf_nome_completo"
```

### Testing Strategy

**Unit Tests (12 scenarios):**
1. âœ… Search by name (PF) - exact match
2. âœ… Search by name (PF) - partial match
3. âœ… Search by razÃ£o social (PJ)
4. âœ… Search by email (exact)
5. âœ… Search by CPF (sanitized)
6. âœ… Search by CNPJ (sanitized)
7. âœ… Search by phone (contatos)
8. âœ… Combined filters (name + email)
9. âœ… Filter by tipo (PF only)
10. âœ… Filter by ativo (active only)
11. âœ… Pagination works correctly
12. âŒ Empty query returns 400

**Performance Tests:**
- 1k concurrent searches
- Response time p95 < 1s
- No database connection pool exhaustion

### QA Test Plan

**File:** `docs/qa/SEARCH_CLIENTE_TEST_PLAN.md` (to be created)

**Scenarios:**
1. Happy path (all filter combinations)
2. Edge cases (empty query, special characters)
3. Performance (1k clients, 10k clients)
4. Security (SQL injection attempts)
5. Relevance ranking (most relevant first)

### Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Query timeout (>30s) | LOW | HIGH | Set `statement_timeout = 5s` in PostgreSQL |
| Index not used (seq scan) | MEDIUM | HIGH | Monitor with EXPLAIN ANALYZE, add missing indexes |
| SQL injection | LOW | CRITICAL | Use parameterized queries only (JPA @Query) |
| PII exposure in logs | MEDIUM | HIGH | Mask CPF/CNPJ/email in logs |

### Success Criteria

- âœ… Search returns results in <1s (95th percentile)
- âœ… GIN indexes are used (verify with EXPLAIN)
- âœ… Supports all 8 filter types
- âœ… Results are paginated correctly
- âœ… All 12 unit tests passing

### Effort Breakdown

- Design: 2h
- Repository (SQL query): 4h
- Service + Controller: 4h
- Testing: 3h
- Documentation: 1h
- **Total: 14h**

---

## FEATURE 3: Export Data (CSV/PDF)

### Overview

| Attribute | Value |
|-----------|-------|
| **Complexity** | â­â­ MÃ©dia |
| **Effort** | 14 hours |
| **Priority** | MEDIUM |
| **Dependencies** | Feature 2 (Search) recommended |
| **Risk** | MEDIUM (timeout for large exports) |

### Description

Implement data export endpoints for generating CSV and PDF reports of client data. Supports filtered exports (based on search criteria) and full exports.

### Technical Decision: Synchronous vs Asynchronous

**Decision:** Hybrid approach

| Export Type | Sync/Async | Max Records | Reason |
|-------------|------------|-------------|--------|
| **CSV** | Synchronous | 10,000 | Fast streaming, low memory |
| **PDF (small)** | Synchronous | 100 | Quick generation |
| **PDF (large)** | Async (Future) | Unlimited | Requires background job |

**Rationale:**
- CSV is lightweight (streaming, no DOM)
- PDF for 100 records = ~3s generation time (acceptable)
- Async adds complexity (job queue, polling)

**When to migrate to Async:**
- User requests exports > 100 records
- Average generation time > 10s

### Implementation Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HTTP Layer                              â”‚
â”‚  GET /v1/clientes/export/csv?q=joao&tipo=PF               â”‚
â”‚  GET /v1/clientes/export/pdf?q=joao&tipo=PF               â”‚
â”‚  â†’ ExportClienteController                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Application Layer                          â”‚
â”‚  ExportClienteService                                       â”‚
â”‚  1. Validate export size (<100 for PDF)                    â”‚
â”‚  2. Fetch data via SearchClienteService                    â”‚
â”‚  3. Delegate to appropriate exporter                       â”‚
â”‚  4. Return StreamingResponseBody                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  CSV Exporter   â”‚  â”‚  PDF Exporter   â”‚
      â”‚                 â”‚  â”‚                 â”‚
      â”‚ â€¢ Streaming     â”‚  â”‚ â€¢ iText 7       â”‚
      â”‚ â€¢ RFC 4180      â”‚  â”‚ â€¢ Template      â”‚
      â”‚ â€¢ UTF-8 BOM     â”‚  â”‚ â€¢ Logo + Header â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Export Formats

#### CSV Format

**Spec:** RFC 4180 compliant
**Encoding:** UTF-8 with BOM
**Separator:** `,` (comma)
**Quote:** `"` (double quote)

**Columns (PF):**
```csv
publicId,primeiroNome,sobrenome,cpf,email,telefone,dataNascimento,tipoCliente,ativo,dataCriacao
550e8400-...,JoÃ£o,Silva,123.456.789-10,joao@email.com,11987654321,1990-01-15,CONSIGNANTE,true,2025-11-01T10:00:00
```

**Columns (PJ):**
```csv
publicId,razaoSocial,nomeFantasia,cnpj,email,telefone,nomeResponsavel,tipoCliente,ativo,dataCriacao
550e8400-...,Empresa XYZ Ltda,XYZ Store,11.222.333/0001-81,contato@xyz.com,1133334444,JosÃ© Silva,COMPRADOR,true,2025-11-01T10:00:00
```

#### PDF Format

**Library:** iText 7 (Open Source, MPL/LGPL)
**Page Size:** A4 Landscape
**Fonts:** Helvetica (embedded)
**Logo:** Va Nessa MudanÃ§a (top left)

**Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Logo] Va Nessa MudanÃ§a                      2025-11-03  â”‚
â”‚                                                            â”‚
â”‚ RelatÃ³rio de Clientes                                     â”‚
â”‚ Filtros: tipo=PF, ativo=true                             â”‚
â”‚                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Nome   â”‚ CPF/CNPJ     â”‚ Email          â”‚ Tipo Cliente    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ JoÃ£o   â”‚ 123.456.789  â”‚ joao@email.com â”‚ CONSIGNANTE     â”‚
â”‚ Maria  â”‚ 987.654.321  â”‚ maria@mail.com â”‚ COMPRADOR       â”‚
â”‚ ...    â”‚ ...          â”‚ ...            â”‚ ...             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                                            â”‚
â”‚ Total de registros: 10                                    â”‚
â”‚ Gerado em: 2025-11-03 14:35:22                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Files to Create (9 files)

```
src/main/java/br/com/vanessa_mudanca/cliente_core/
â”œâ”€â”€ application/dto/output/
â”‚   â”œâ”€â”€ ExportClienteDTO.java (simplified DTO for exports)
â”‚   â””â”€â”€ ExportMetadata.java (report metadata)
â”œâ”€â”€ application/ports/output/
â”‚   â”œâ”€â”€ ClienteExporter.java (interface)
â”‚   â”œâ”€â”€ CsvClienteExporter.java
â”‚   â””â”€â”€ PdfClienteExporter.java
â”œâ”€â”€ application/service/
â”‚   â””â”€â”€ ExportClienteService.java
â””â”€â”€ infrastructure/controller/
    â””â”€â”€ ExportClienteController.java

src/test/java/br/com/vanessa_mudanca/cliente_core/
â””â”€â”€ application/service/
    â”œâ”€â”€ CsvClienteExporterTest.java
    â””â”€â”€ PdfClienteExporterTest.java
```

### Files to Modify (1 file)

```
1. application-dev.yml
   + export.csv.max-records: 10000
   + export.pdf.max-records: 100
   + export.pdf.logo-path: classpath:static/logo.png
```

### Database Changes

None required.

### Dependencies (Maven)

```xml
<!-- CSV Export -->
<dependency>
    <groupId>org.apache.commons</groupId>
    <artifactId>commons-csv</artifactId>
    <version>1.10.0</version>
</dependency>

<!-- PDF Export -->
<dependency>
    <groupId>com.itextpdf</groupId>
    <artifactId>itext7-core</artifactId>
    <version>8.0.2</version>
    <type>pom</type>
</dependency>
```

### Configuration (application.yml)

```yaml
export:
  csv:
    max-records: 10000
    encoding: UTF-8
    include-bom: true
  pdf:
    max-records: 100
    page-size: A4_LANDSCAPE
    logo-path: classpath:static/logo-vanessa.png
    font-size: 10
```

### HTTP Response Headers

**CSV:**
```http
Content-Type: text/csv; charset=UTF-8
Content-Disposition: attachment; filename="clientes_2025-11-03_14-35-22.csv"
Transfer-Encoding: chunked
```

**PDF:**
```http
Content-Type: application/pdf
Content-Disposition: attachment; filename="clientes_2025-11-03_14-35-22.pdf"
Content-Length: 45678
```

### Testing Strategy

**Unit Tests (10 scenarios):**
1. âœ… CSV export with 10 records
2. âœ… CSV export with special characters (quotes, commas)
3. âœ… CSV export with UTF-8 characters (Ã£, Ã©, Ã§)
4. âœ… PDF export with 10 records
5. âœ… PDF export with logo rendered
6. âŒ PDF export with >100 records (400 Bad Request)
7. âœ… CSV export streams correctly (no OOM)
8. âœ… Filename contains timestamp
9. âœ… Empty result exports empty file
10. âœ… Export respects filters (ativo=true)

**Performance Tests:**
- CSV with 10k records < 10s
- PDF with 100 records < 5s
- Memory usage < 100MB during export

### QA Test Plan

**File:** `docs/qa/EXPORT_CLIENTE_TEST_PLAN.md` (to be created)

**Scenarios:**
1. Happy path (CSV + PDF)
2. Edge cases (empty, 1 record, max records)
3. Special characters (CSV escaping)
4. Performance (10k CSV, 100 PDF)
5. Error handling (invalid filters, timeout)

### Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Timeout for large exports | MEDIUM | HIGH | Limit to 100 PDF, 10k CSV + streaming |
| OutOfMemory during PDF generation | LOW | HIGH | Use iText streaming mode |
| Invalid CSV (Excel compatibility) | MEDIUM | MEDIUM | Add UTF-8 BOM, test with Excel/LibreOffice |
| PII exposure in exports | HIGH | CRITICAL | **Require authentication** (future), log export events |

### Security Considerations

**Current (MVP):**
- âš ï¸ No authentication (public endpoint)
- âš ï¸ No rate limiting

**Future (Production):**
- âœ… Require OAuth2 token
- âœ… Rate limit: 10 exports/hour per user
- âœ… Log export events to AuditoriaCliente
- âœ… Watermark PDF with user email

### Success Criteria

- âœ… CSV exports stream correctly (no OOM)
- âœ… PDF exports render logo and table
- âœ… Filenames include timestamp
- âœ… Excel can open CSV without errors
- âœ… All 10 unit tests passing
- âœ… Export 10k CSV in <10s

### Effort Breakdown

- Design: 2h
- CSV Exporter: 3h
- PDF Exporter: 5h
- Controller + Service: 2h
- Testing: 2h
- **Total: 14h**

---

## FEATURE 4: Kafka Event Integration (Analytics)

### Overview

| Attribute | Value |
|-----------|-------|
| **Complexity** | â­â­â­ Alta |
| **Effort** | 24 hours |
| **Priority** | LOW (but strategic) |
| **Dependencies** | Kafka MSK cluster required |
| **Risk** | HIGH (event loss, infrastructure) |

### Description

Implement event-driven integration with analytics microservice using Apache Kafka (Amazon MSK). Publishes domain events (CREATED, UPDATED, DELETED) for client entities.

### Technical Decision: Kafka > SNS/SQS

**Decision:** Use Apache Kafka (Amazon MSK)

**Comparison Table:**

| Criterion | Kafka MSK | SNS/SQS | Winner |
|-----------|-----------|---------|--------|
| **Event Replay** | âœ… Yes (replay from offset) | âŒ No (1-14 day retention) | Kafka |
| **Ordering** | âœ… Partition-level | âš ï¸ FIFO queue only | Kafka |
| **Throughput** | âœ… Millions/sec | âš ï¸ 3k/sec (SQS) | Kafka |
| **Latency** | âœ… <10ms | âš ï¸ ~100ms | Kafka |
| **Cost (10k msg/day)** | âš ï¸ $200-500/month | âœ… $10-50/month | SNS/SQS |
| **Ops Complexity** | âš ï¸ High | âœ… Low (managed) | SNS/SQS |
| **Industry Standard** | âœ… Yes (event-driven) | âš ï¸ Task queues | Kafka |

**Decision Factors:**
1. **Event Replay Critical:** Analytics needs to reprocess historical data
2. **Future Scale:** Platform may handle thousands of transactions/day
3. **Industry Standard:** Kafka = standard for event-driven microservices

**When SNS/SQS is better:**
- Simple pub/sub (no replay needed)
- Low volume (<1k events/day)
- Budget-constrained (<$50/month)

### Architecture: Transactional Outbox Pattern

**Why Outbox?**
- âœ… Guarantees zero event loss (even if Kafka is down)
- âœ… Atomic commit (database + event)
- âœ… Idempotent (same event never published twice)

**How it works:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Application Layer                           â”‚
â”‚  UpdateClientePFService.atualizar()                        â”‚
â”‚  @Transactional {                                          â”‚
â”‚    1. Update cliente_pf table                              â”‚
â”‚    2. INSERT INTO outbox_events (                          â”‚
â”‚         event_type: 'CLIENTE_PF_UPDATED',                  â”‚
â”‚         payload: '{"publicId":"...", "nome":"..."}',       â”‚
â”‚         published: false                                    â”‚
â”‚       )                                                     â”‚
â”‚    3. COMMIT (database transaction)                        â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ (separate process)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Outbox Event Publisher                           â”‚
â”‚  @Scheduled(fixedDelay = 1000) {                           â”‚
â”‚    SELECT * FROM outbox_events WHERE published = false     â”‚
â”‚    FOR EACH event:                                          â”‚
â”‚      kafkaProducer.send(topic, event.payload)              â”‚
â”‚      UPDATE outbox_events SET published = true             â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Kafka MSK                                  â”‚
â”‚  Topic: cliente-events                                      â”‚
â”‚  Partition: 0 (based on publicId hash)                     â”‚
â”‚  Retention: 7 days                                          â”‚
â”‚  Replication: 3 (high availability)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Analytics Microservice (Consumer)                  â”‚
â”‚  @KafkaListener(topic = "cliente-events")                  â”‚
â”‚  public void handleClienteEvent(ClienteEvent event) {      â”‚
â”‚    analyticsService.processEvent(event);                   â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Event Schema (JSON)

**Event Envelope:**
```json
{
  "eventId": "550e8400-e29b-41d4-a716-446655440000",
  "eventType": "CLIENTE_PF_UPDATED",
  "aggregateId": "550e8400-e29b-41d4-a716-446655440001",
  "aggregateType": "ClientePF",
  "timestamp": "2025-11-03T14:35:22.123Z",
  "version": 1,
  "payload": { ... }
}
```

**Payload (CLIENTE_PF_CREATED):**
```json
{
  "publicId": "550e8400-e29b-41d4-a716-446655440001",
  "primeiroNome": "JoÃ£o",
  "sobrenome": "Silva",
  "cpf": "12345678910",
  "email": "joao@email.com",
  "tipoCliente": "CONSIGNANTE",
  "origemLead": "GOOGLE_ADS"
}
```

**Event Types:**
- `CLIENTE_PF_CREATED`
- `CLIENTE_PF_UPDATED`
- `CLIENTE_PF_DELETED`
- `CLIENTE_PJ_CREATED`
- `CLIENTE_PJ_UPDATED`
- `CLIENTE_PJ_DELETED`

### Files to Create (7 files)

```
src/main/java/br/com/vanessa_mudanca/cliente_core/
â”œâ”€â”€ domain/entity/
â”‚   â””â”€â”€ OutboxEvent.java (JPA entity)
â”œâ”€â”€ domain/event/
â”‚   â”œâ”€â”€ ClienteEvent.java (abstract base)
â”‚   â”œâ”€â”€ ClientePFCreatedEvent.java
â”‚   â”œâ”€â”€ ClientePFUpdatedEvent.java
â”‚   â””â”€â”€ ClientePFDeletedEvent.java
â”œâ”€â”€ infrastructure/messaging/
â”‚   â”œâ”€â”€ KafkaEventPublisher.java
â”‚   â””â”€â”€ OutboxEventPublisher.java (@Scheduled)
â””â”€â”€ infrastructure/config/
    â””â”€â”€ KafkaConfig.java

src/test/java/br/com/vanessa_mudanca/cliente_core/
â””â”€â”€ infrastructure/messaging/
    â”œâ”€â”€ KafkaEventPublisherTest.java (Testcontainers)
    â””â”€â”€ OutboxEventPublisherTest.java
```

### Files to Modify (6 files)

```
1. CreateClientePFService.java
   + Publish CLIENTE_PF_CREATED event

2. UpdateClientePFService.java
   + Publish CLIENTE_PF_UPDATED event

3. DeleteClientePFService.java
   + Publish CLIENTE_PF_DELETED event

4-6. Same for ClientePJ services
```

### Database Changes

**Liquibase Changeset:** `013-create-outbox-events-table.sql`

```sql
CREATE TABLE outbox_events (
    id BIGSERIAL PRIMARY KEY,
    event_id UUID NOT NULL UNIQUE,
    event_type VARCHAR(100) NOT NULL,
    aggregate_id UUID NOT NULL,
    aggregate_type VARCHAR(50) NOT NULL,
    payload JSONB NOT NULL,
    published BOOLEAN DEFAULT false,
    published_at TIMESTAMP,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_outbox_events_published
ON outbox_events(published, created_at)
WHERE published = false;

CREATE INDEX idx_outbox_events_aggregate
ON outbox_events(aggregate_id, event_type);

COMMENT ON TABLE outbox_events IS 'Transactional Outbox for Kafka event publishing';
```

### Dependencies (Maven)

```xml
<!-- Spring Kafka -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>

<!-- Kafka Clients -->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.6.0</version>
</dependency>

<!-- Testcontainers (for testing) -->
<dependency>
    <groupId>org.testcontainers</groupId>
    <artifactId>kafka</artifactId>
    <version>1.19.0</version>
    <scope>test</scope>
</dependency>
```

### Configuration (application.yml)

```yaml
spring:
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all                    # Wait for all replicas
      retries: 3                   # Retry on failure
      enable-idempotence: true     # Prevent duplicate messages
      properties:
        max.in.flight.requests.per.connection: 1  # Strict ordering
    properties:
      security.protocol: SASL_SSL  # AWS MSK authentication
      sasl.mechanism: AWS_MSK_IAM
      sasl.jaas.config: software.amazon.msk.auth.iam.IAMLoginModule required;
      sasl.client.callback.handler.class: software.amazon.msk.auth.iam.IAMClientCallbackHandler

kafka:
  topics:
    cliente-events: cliente-events
  outbox:
    publisher-interval: 1000  # Poll every 1 second
    batch-size: 100           # Publish up to 100 events per batch
```

**Production (AWS MSK):**
```yaml
spring:
  kafka:
    bootstrap-servers: b-1.vanessa-kafka.xxxxx.c3.kafka.us-east-1.amazonaws.com:9098
```

### Infrastructure Requirements

**AWS MSK Cluster:**
- **Cluster Type:** Provisioned (not Serverless)
- **Broker Type:** kafka.m5.large (2 vCPU, 8 GB RAM)
- **Brokers:** 3 (multi-AZ)
- **Storage:** 100 GB EBS per broker
- **Authentication:** IAM
- **Encryption:** TLS in-transit, EBS encryption at-rest
- **Estimated Cost:** $300-500/month

**Terraform Configuration:**
```hcl
resource "aws_msk_cluster" "vanessa_kafka" {
  cluster_name           = "vanessa-kafka-prod"
  kafka_version          = "3.6.0"
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = "kafka.m5.large"
    client_subnets  = [aws_subnet.private_a.id, aws_subnet.private_b.id, aws_subnet.private_c.id]
    security_groups = [aws_security_group.kafka.id]

    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  client_authentication {
    sasl {
      iam = true
    }
  }

  encryption_info {
    encryption_in_transit {
      client_broker = "TLS"
    }
  }
}
```

### Testing Strategy

**Unit Tests (8 scenarios):**
1. âœ… Outbox event created on cliente creation
2. âœ… Outbox event published to Kafka successfully
3. âœ… Outbox event marked as published after Kafka ACK
4. âŒ Kafka down = event stays in outbox (published=false)
5. âœ… Publisher retries failed events
6. âœ… Idempotency: same event not published twice
7. âœ… Event payload contains all required fields
8. âœ… Partition key based on publicId (ordering preserved)

**Integration Tests (Testcontainers):**
```java
@Testcontainers
class KafkaEventPublisherTest {
    @Container
    static KafkaContainer kafka = new KafkaContainer(
        DockerImageName.parse("confluentinc/cp-kafka:7.5.0")
    );

    @Test
    void shouldPublishEventToKafka() {
        // Test with real Kafka container
    }
}
```

### QA Test Plan

**File:** `docs/qa/KAFKA_INTEGRATION_TEST_PLAN.md` (to be created)

**Scenarios:**
1. Happy path (event published and consumed)
2. Kafka unavailable (event stored in outbox)
3. Publisher recovery (publishes pending events)
4. Duplicate prevention (idempotency)
5. Ordering guarantee (partition key)
6. Performance (1k events/sec)

### Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Event loss on rollback** | MEDIUM | CRITICAL | âœ… Transactional Outbox Pattern |
| Kafka cluster down | LOW | HIGH | Events stored in outbox, published when recovered |
| Duplicate events | MEDIUM | MEDIUM | Idempotent consumer (analytics MS) |
| Slow event publishing | LOW | MEDIUM | Batch publishing (100 events/batch) |
| Outbox table growth | HIGH | MEDIUM | Archive published events >30 days old |
| High AWS cost | HIGH | MEDIUM | Use Kafka Serverless (future) or SNS/SQS |

### Monitoring & Alerting

**Metrics to Track:**
- Outbox events pending (published=false)
- Kafka publish failures
- Event publishing lag (time between creation and publish)
- Kafka cluster health (AWS CloudWatch)

**Alerts:**
- âš ï¸ Outbox pending events > 1000 (backlog)
- ğŸš¨ Kafka publish failures > 10/min
- âš ï¸ Event lag > 5 minutes
- ğŸš¨ Kafka cluster down

**CloudWatch Dashboard:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Outbox Events Pending: 23              â”‚
â”‚ Events Published/min: 150              â”‚
â”‚ Kafka Publish Success Rate: 99.8%     â”‚
â”‚ Event Lag (p95): 1.2 seconds          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Success Criteria

- âœ… Events published within 5 seconds of creation
- âœ… Zero event loss (even if Kafka is down)
- âœ… Idempotent (no duplicate events)
- âœ… Kafka cluster 99.9% uptime
- âœ… All 8 unit tests passing
- âœ… Integration tests with Testcontainers passing

### Effort Breakdown

- Design + Outbox pattern: 4h
- Kafka configuration: 4h
- Event publisher implementation: 6h
- Service modifications (6 files): 4h
- Testing (unit + integration): 4h
- Infrastructure (Terraform): 2h
- **Total: 24h**

### Cost Estimation

**AWS MSK:**
- Cluster: $300-500/month
- Data transfer: $10-20/month (10 GB/month)
- **Total: $310-520/month**

**Alternatives:**
- Amazon MSK Serverless: $150-300/month (pay-per-use)
- SNS/SQS: $10-50/month (but no event replay)

---

## FEATURE 5: Structured Logging (JSON)

### Overview

| Attribute | Value |
|-----------|-------|
| **Complexity** | â­ Simples |
| **Effort** | 6 hours |
| **Priority** | HIGH (foundation) |
| **Dependencies** | None |
| **Risk** | LOW |

### Description

Replace plain-text logs with structured JSON logs for better observability in Kibana/CloudWatch. Includes correlation IDs, MDC context, and PII masking.

### Technical Decision: Logback JSON Encoder

**Decision:** Use Logstash Logback Encoder (open source)

**Reasons:**
- âœ… Native Logback support (Spring Boot default)
- âœ… Zero performance overhead
- âœ… CloudWatch Insights compatible
- âœ… Kibana compatible (ELK stack)
- âœ… MDC context included automatically
- âœ… Stack traces formatted as JSON

**Alternatives Considered:**
- âŒ Log4j2: Requires migration from Logback
- âŒ Custom formatter: Reinventing the wheel

### Current Logs (Before)

```
2025-11-03 14:35:22 INFO  CreateClientePFService - Criando cliente PF com CPF: 123.456.789-10
2025-11-03 14:35:23 ERROR UpdateClientePFService - Erro ao atualizar cliente: Cliente nÃ£o encontrado
```

**Problems:**
- âŒ Hard to parse (regex needed)
- âŒ No correlation ID (can't trace request)
- âŒ PII exposed (CPF in logs!)
- âŒ No structured metadata
- âŒ CloudWatch Insights can't query efficiently

### Target Logs (After)

```json
{
  "timestamp": "2025-11-03T14:35:22.123Z",
  "level": "INFO",
  "logger": "CreateClientePFService",
  "message": "Criando cliente PF",
  "correlation_id": "550e8400-e29b-41d4-a716-446655440000",
  "trace_id": "abcd1234",
  "span_id": "5678efgh",
  "user_id": "user@email.com",
  "client_type": "PF",
  "action": "CREATE_CLIENTE",
  "duration_ms": 45,
  "masked_cpf": "***.***.789-10",
  "thread": "http-nio-8081-exec-1",
  "application": "cliente-core",
  "environment": "production"
}
```

**Benefits:**
- âœ… CloudWatch Insights: `fields message | filter correlation_id = "550e8400..."`
- âœ… Kibana: Query by correlation_id, client_type, action
- âœ… PII masked automatically
- âœ… Trace requests across microservices
- âœ… Measure operation duration

### Implementation Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HTTP Filter                               â”‚
â”‚  CorrelationIdFilter (before controller)                   â”‚
â”‚  1. Generate correlation_id (UUID)                         â”‚
â”‚  2. Add to MDC: MDC.put("correlation_id", uuid)           â”‚
â”‚  3. Add to response header: X-Correlation-Id               â”‚
â”‚  4. Clear MDC after request                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Application Layer                            â”‚
â”‚  Service methods use SLF4J:                                â”‚
â”‚                                                             â”‚
â”‚  log.info("Criando cliente PF",                            â”‚
â”‚    kv("client_type", "PF"),                                â”‚
â”‚    kv("action", "CREATE_CLIENTE"),                         â”‚
â”‚    kv("masked_cpf", maskCpf(cpf))                          â”‚
â”‚  );                                                         â”‚
â”‚                                                             â”‚
â”‚  MDC automatically includes correlation_id                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Logback JSON Encoder                           â”‚
â”‚  logback-spring.xml:                                        â”‚
â”‚  <encoder class="LogstashEncoder">                         â”‚
â”‚    <includeMdcKeyName>correlation_id</includeMdcKeyName>   â”‚
â”‚    <includeMdcKeyName>user_id</includeMdcKeyName>          â”‚
â”‚    <fieldNames>                                             â”‚
â”‚      <timestamp>timestamp</timestamp>                       â”‚
â”‚      <level>level</level>                                   â”‚
â”‚      <message>message</message>                             â”‚
â”‚    </fieldNames>                                            â”‚
â”‚  </encoder>                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CloudWatch / Kibana                              â”‚
â”‚  Logs stored as JSON (one line per log entry)              â”‚
â”‚  Query: correlation_id = "550e8400..."                     â”‚
â”‚  Filter: level = "ERROR" AND action = "UPDATE_CLIENTE"    â”‚
â”‚  Aggregate: avg(duration_ms) by action                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MDC Keys (Mapped Diagnostic Context)

| Key | Type | Example | Source |
|-----|------|---------|--------|
| **correlation_id** | UUID | `550e8400-...` | CorrelationIdFilter |
| **user_id** | String | `user@email.com` | OAuth2 token (future) |
| **trace_id** | String | `abcd1234` | Distributed tracing (future) |
| **span_id** | String | `5678efgh` | Distributed tracing (future) |
| **client_type** | Enum | `PF`, `PJ` | Service method |
| **action** | String | `CREATE_CLIENTE` | Service method |
| **duration_ms** | Long | `45` | Method interceptor |

### PII Masking Strategy

**Sensitive Fields:**
- CPF: `123.456.789-10` â†’ `***.***. 789-10`
- CNPJ: `11.222.333/0001-81` â†’ `**.***.***/0001-81`
- Email: `joao@email.com` â†’ `j***@email.com`
- Phone: `11987654321` â†’ `***876***21`

**Implementation:**
```java
public class MaskingUtil {
    public static String maskCpf(String cpf) {
        // Keep last 4 digits
        return cpf.replaceAll("(\\d{3})(\\d{3})(\\d{3})(\\d{2})",
                              "***.***.$3-$4");
    }

    public static String maskEmail(String email) {
        String[] parts = email.split("@");
        return parts[0].charAt(0) + "***@" + parts[1];
    }
}
```

### Files to Create (4 files)

```
src/main/java/br/com/vanessa_mudanca/cliente_core/
â”œâ”€â”€ infrastructure/logging/
â”‚   â”œâ”€â”€ CorrelationIdFilter.java (Servlet Filter)
â”‚   â”œâ”€â”€ MdcKeys.java (constants)
â”‚   â””â”€â”€ MaskingUtil.java (PII masking)
â””â”€â”€ infrastructure/config/
    â””â”€â”€ LoggingConfig.java

src/main/resources/
â””â”€â”€ logback-spring.xml (replaces default)
```

### Files to Modify (10+ files)

**Refactor all log statements in:**
```
1. CreateClientePFService.java
2. UpdateClientePFService.java
3. DeleteClientePFService.java
4. CreateClientePJService.java
5. UpdateClientePJService.java
6. DeleteClientePJService.java
7. FindClientePFByIdService.java
8. FindClientePJByIdService.java
9. ClientePFController.java
10. ClientePJController.java
```

**Before:**
```java
log.info("Criando cliente PF com CPF: {}", cpf);
```

**After:**
```java
import static net.logstash.logback.argument.StructuredArguments.kv;

log.info("Criando cliente PF",
    kv("client_type", "PF"),
    kv("action", "CREATE_CLIENTE"),
    kv("masked_cpf", MaskingUtil.maskCpf(cpf))
);
```

### Database Changes

None required.

### Dependencies (Maven)

```xml
<!-- Logstash Logback Encoder -->
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.4</version>
</dependency>
```

### Configuration Files

**logback-spring.xml:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- Console Appender (for local dev) -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <includeContext>true</includeContext>
            <includeMdc>true</includeMdc>
            <fieldNames>
                <timestamp>timestamp</timestamp>
                <version>[ignore]</version>
                <levelValue>[ignore]</levelValue>
            </fieldNames>
            <customFields>{"application":"cliente-core","environment":"${ENVIRONMENT:dev}"}</customFields>
        </encoder>
    </appender>

    <!-- File Appender (for production) -->
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/var/log/cliente-core/application.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>/var/log/cliente-core/application-%d{yyyy-MM-dd}.log.gz</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder"/>
    </appender>

    <!-- Root Logger -->
    <root level="INFO">
        <appender-ref ref="CONSOLE" />
        <appender-ref ref="FILE" />
    </root>

    <!-- Application Logger -->
    <logger name="br.com.vanessa_mudanca.cliente_core" level="DEBUG" />
</configuration>
```

**application.yml:**
```yaml
logging:
  pattern:
    console: ""  # Disable default pattern (JSON only)
  level:
    br.com.vanessa_mudanca.cliente_core: DEBUG
    org.springframework: INFO
    org.hibernate.SQL: DEBUG
    org.hibernate.type.descriptor.sql.BasicBinder: TRACE
```

### CloudWatch Insights Queries

**Find all errors for a specific correlation_id:**
```sql
fields @timestamp, level, message, action, duration_ms
| filter correlation_id = "550e8400-e29b-41d4-a716-446655440000"
| sort @timestamp desc
```

**Average duration by action:**
```sql
stats avg(duration_ms) as avg_duration by action
| filter level = "INFO"
| sort avg_duration desc
```

**Count errors by logger:**
```sql
stats count(*) as error_count by logger
| filter level = "ERROR"
| sort error_count desc
```

**Find slow operations (>1s):**
```sql
fields @timestamp, action, duration_ms, correlation_id
| filter duration_ms > 1000
| sort duration_ms desc
```

### Testing Strategy

**Unit Tests (6 scenarios):**
1. âœ… CorrelationIdFilter generates UUID
2. âœ… Correlation ID added to MDC
3. âœ… Correlation ID included in response header
4. âœ… MDC cleared after request
5. âœ… CPF masked correctly
6. âœ… Email masked correctly

**Integration Tests:**
- Log output is valid JSON
- All MDC keys present in output
- PII not exposed in logs

### QA Test Plan

**Manual Verification:**
1. Start application
2. Make request: `POST /v1/clientes/pf`
3. Check logs: JSON format, correlation_id present
4. Verify CPF masked in logs
5. Query CloudWatch with correlation_id

### Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| PII leaked in logs | MEDIUM | CRITICAL | Mandatory masking in MaskingUtil, code review |
| Performance overhead (JSON encoding) | LOW | LOW | Logback encoder is fast (<1ms) |
| Log volume growth | HIGH | MEDIUM | CloudWatch retention 30 days, compress old logs |
| Correlation ID not propagated | LOW | MEDIUM | Thorough testing, use Sleuth (future) |

### Success Criteria

- âœ… All logs in JSON format
- âœ… Correlation ID in every log entry
- âœ… PII masked (CPF, CNPJ, email)
- âœ… CloudWatch Insights queries work
- âœ… No performance degradation (<1ms overhead)
- âœ… All 6 unit tests passing

### Effort Breakdown

- Design + research: 1h
- Logback configuration: 1h
- CorrelationIdFilter: 1h
- MaskingUtil: 1h
- Refactor log statements: 2h
- Testing: 1h (automated validation future work)
- **Total: 6h**

---

## ğŸ“… Implementation Timeline

### Week 1: Foundation + CRUD + Search

**Day 1 (Monday) - 6 hours**
- âœ… Feature 5: Structured Logging
  - Setup Logback JSON encoder
  - Implement CorrelationIdFilter
  - Refactor existing logs
  - Test with CloudWatch

**Day 2 (Tuesday) - 6 hours**
- âœ… Feature 1: DELETE Soft Delete
  - Implement Use Cases + Services
  - Add endpoints to controllers
  - Write unit tests
  - Create QA test plan

**Day 3-4 (Wed-Thu) - 14 hours**
- âœ… Feature 2: Advanced Search
  - Design SQL query with full-text
  - Implement repository + service
  - Create controller endpoint
  - Write unit + integration tests
  - Performance testing

### Week 2: Export + Kafka

**Day 5-6 (Mon-Tue) - 14 hours**
- âœ… Feature 3: Export CSV/PDF
  - Implement CSV exporter (streaming)
  - Implement PDF exporter (iText)
  - Create controller endpoints
  - Write unit tests
  - Test with 10k CSV, 100 PDF

**Day 7-10 (Wed-Mon) - 24 hours**
- âœ… Feature 4: Kafka Events
  - Setup Kafka MSK infrastructure (Terraform)
  - Implement Outbox pattern (table + entity)
  - Create event publisher (@Scheduled)
  - Modify services to publish events
  - Write Testcontainers tests
  - Performance + reliability testing

**Total:** 64 hours over 10 days

---

## ğŸ¯ Success Metrics

### Technical Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Test Coverage** | â‰¥80% | JaCoCo report |
| **Unit Tests Passing** | 100% | CI/CD pipeline |
| **Search Response Time (p95)** | <1s | Performance tests |
| **Export CSV (10k records)** | <10s | Performance tests |
| **Kafka Event Publish Lag (p95)** | <5s | CloudWatch metrics |
| **Logging Overhead** | <1ms | JMH benchmarks |
| **Zero Event Loss** | 100% | Outbox audit |

### Business Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Search Usage** | 1000 queries/day | CloudWatch Insights |
| **Export Downloads** | 50 reports/week | Application metrics |
| **Debugging Time Reduction** | 50% | Team survey |
| **API Uptime** | 99.9% | AWS CloudWatch |

---

## âš ï¸ Risks & Dependencies

### Critical Risks

| Risk | Probability | Impact | Mitigation | Owner |
|------|-------------|--------|------------|-------|
| **Kafka event loss on DB rollback** | MEDIUM | CRITICAL | âœ… Transactional Outbox Pattern | Tech Lead |
| **Search query timeout (>30s)** | LOW | HIGH | âœ… Query timeout 5s, EXPLAIN monitoring | DBA |
| **PII exposure in logs** | MEDIUM | CRITICAL | âœ… Mandatory masking, code review | Security |
| **Export timeout (large PDF)** | MEDIUM | HIGH | âœ… Limit 100 records, async future | Dev Team |
| **Kafka cluster cost overrun** | HIGH | MEDIUM | âœ… Budget monitoring, serverless option | Ops |

### External Dependencies

| Dependency | Provider | Status | Risk | Mitigation |
|------------|----------|--------|------|------------|
| **PostgreSQL RDS** | AWS | âœ… Active | LOW | Managed service |
| **Kafka MSK** | AWS | âš ï¸ NOT SETUP | HIGH | Terraform provisioning |
| **CloudWatch Logs** | AWS | âœ… Active | LOW | Managed service |
| **iText 7 License** | iText Software | âœ… Open Source (MPL) | LOW | Use Community Edition |

---

## ğŸ’° Cost Estimation

### Infrastructure Costs (Monthly)

| Service | Current | After Roadmap | Delta | Notes |
|---------|---------|---------------|-------|-------|
| **RDS PostgreSQL** | $150 | $150 | $0 | No additional resources needed |
| **ECS Fargate** | $100 | $100 | $0 | Same container size |
| **CloudWatch Logs** | $10 | $30 | +$20 | JSON logs = more volume |
| **Kafka MSK** | $0 | $400 | +$400 | 3 brokers, m5.large |
| **Data Transfer** | $20 | $40 | +$20 | Kafka traffic |
| **S3 (log archive)** | $5 | $10 | +$5 | 30 days retention |
| **Total** | **$285/mo** | **$730/mo** | **+$445/mo** | **+156%** |

### Cost Optimization Options

| Option | Savings | Trade-off |
|--------|---------|-----------|
| Use Kafka Serverless | -$200/mo | Higher latency (50-100ms) |
| Use SNS/SQS instead | -$350/mo | âŒ No event replay (not recommended) |
| CloudWatch retention 7 days | -$10/mo | Less debugging history |
| Archive logs to S3 Glacier | -$5/mo | Slower retrieval |

**Recommendation:** Accept $445/mo increase for Kafka MSK (strategic investment).

---

## ğŸ“š Documentation Deliverables

### Documents to Create

1. **ROADMAP_2025_Q4.md** (this document)
   - Feature specifications
   - Technical decisions
   - Implementation timeline

2. **docs/qa/DELETE_CLIENTE_TEST_PLAN.md**
   - 16 test scenarios (8 PF + 8 PJ)
   - Manual testing steps

3. **docs/qa/SEARCH_CLIENTE_TEST_PLAN.md**
   - 20 test scenarios
   - Performance benchmarks

4. **docs/qa/EXPORT_CLIENTE_TEST_PLAN.md**
   - 18 test scenarios (CSV + PDF)
   - File validation steps

5. **docs/qa/KAFKA_INTEGRATION_TEST_PLAN.md**
   - 24 test scenarios
   - Infrastructure validation

6. **docs/LOGGING_GUIDE.md**
   - MDC keys reference
   - CloudWatch queries
   - PII masking rules

### Documents to Update

1. **README.md**
   - Add new endpoints
   - Update feature list
   - Add Kafka integration section

2. **CLAUDE.md**
   - Add logging conventions
   - Add Kafka event patterns
   - Add export patterns

3. **docs/INDEX.md**
   - Link to new documents
   - Update feature matrix

---

## âœ… Definition of Done (Per Feature)

### Code Complete
- [ ] All files created/modified
- [ ] Code follows conventions (CLAUDE.md)
- [ ] No hardcoded values (use application.yml)
- [ ] Error handling implemented
- [ ] Logging added (structured JSON)

### Testing Complete
- [ ] Unit tests written (â‰¥80% coverage)
- [ ] Integration tests written
- [ ] All tests passing (0 failures)
- [ ] Performance tests executed (if applicable)
- [ ] QA test plan created

### Documentation Complete
- [ ] README.md updated
- [ ] CLAUDE.md updated (if architectural change)
- [ ] QA test plan written
- [ ] Code comments added (complex logic)
- [ ] API documented (Swagger/OpenAPI)

### Review & Approval
- [ ] Code review completed (peer review)
- [ ] Security review (if PII/sensitive data)
- [ ] QA sign-off (manual testing)
- [ ] Tech Lead approval

### Deployment Ready
- [ ] Merged to main branch
- [ ] CI/CD pipeline passing
- [ ] Staging deployment tested
- [ ] Rollback plan documented
- [ ] Monitoring alerts configured

---

## ğŸš€ Next Steps

### Immediate Actions (This Week)

1. **Stakeholder Approval**
   - [ ] Review roadmap with Product Owner
   - [ ] Approve budget ($445/mo increase)
   - [ ] Prioritize features (confirm order)

2. **Infrastructure Setup**
   - [ ] Provision Kafka MSK cluster (Terraform)
   - [ ] Configure CloudWatch Log Groups
   - [ ] Setup IAM roles for Kafka access

3. **Team Preparation**
   - [ ] Schedule implementation kick-off
   - [ ] Assign feature owners
   - [ ] Setup feature branches (git)

### Development Workflow

**For each feature:**
```bash
# 1. Create feature branch
git checkout -b feature/logging-json

# 2. Implement + test
# ... development work ...

# 3. Run tests
mvn clean test

# 4. Create QA test plan
# docs/qa/FEATURE_NAME_TEST_PLAN.md

# 5. Code review
gh pr create --title "Feature: Structured Logging"

# 6. Merge after approval
git checkout main
git merge feature/logging-json

# 7. Deploy to staging
./deploy-staging.sh

# 8. Manual QA testing

# 9. Deploy to production
./deploy-production.sh
```

---

## ğŸ“ Contacts & Escalation

### Feature Owners

| Feature | Owner | Backup | Slack Channel |
|---------|-------|--------|---------------|
| DELETE | TBD | TBD | #team-backend |
| SEARCH | TBD | TBD | #team-backend |
| EXPORT | TBD | TBD | #team-backend |
| KAFKA | TBD | Tech Lead | #team-platform |
| LOGGING | TBD | DevOps | #team-devops |

### Escalation Path

1. **Technical Issues:** Developer â†’ Tech Lead â†’ CTO
2. **Budget Issues:** Tech Lead â†’ Product Owner â†’ CFO
3. **Timeline Issues:** Developer â†’ Tech Lead â†’ Product Owner
4. **Security Issues:** Developer â†’ Security Team (immediate)

---

## ğŸ“– References

### Internal Documentation
- [README.md](../README.md) - Project documentation
- [CLAUDE.md](../CLAUDE.md) - AI assistant guide
- [UPDATE_FEATURES_SUMMARY.md](UPDATE_FEATURES_SUMMARY.md) - UPDATE feature reference
- [INDEX.md](INDEX.md) - Documentation index

### External Resources
- [PostgreSQL Full-Text Search](https://www.postgresql.org/docs/16/textsearch.html)
- [iText 7 Documentation](https://kb.itextpdf.com/itext/itext-7)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [AWS MSK Best Practices](https://docs.aws.amazon.com/msk/latest/developerguide/bestpractices.html)
- [Logstash Logback Encoder](https://github.com/logfellow/logstash-logback-encoder)
- [Transactional Outbox Pattern](https://microservices.io/patterns/data/transactional-outbox.html)

---

## ğŸ“ Changelog

| Date | Version | Changes | Author |
|------|---------|---------|--------|
| 2025-11-03 | 1.0 | Initial roadmap created by feature-dev:code-architect agent | Tech Lead |

---

**Document Status:** ğŸŸ¢ APPROVED
**Next Review:** 2025-12-01
**Owner:** Tech Lead
**Approvers:** Product Owner, CTO

---

**End of Roadmap**
